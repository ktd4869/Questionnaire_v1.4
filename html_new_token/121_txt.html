<p id=p0>This post is a part of an ongoing series comparing the performance of <span class='token pointer' id='tok-121-0' style='color:red'>Cloudflare</span>Workers with other Serverless providers. In our past tests we intentionally chose a workload which imposes virtually no <span class='token pointer' id='tok-121-1' style='color:red'>CPU load</span>the current time). For these tests, let's look at something which pushes hardware to the limit: <span class='token pointer' id='tok-121-2' style='color:red'>cryptography.</span></p><p id=p52>Cloudflare Workers are seven times faster than a default <span class='token pointer' id='tok-121-3' style='color:red'>Lambda function</span>workloads which push the <span class='token pointer' id='tok-121-4' style='color:red'>CPU.</span>Workers are six times faster than <span class='token pointer' id='tok-121-5' style='color:red'>Lambda@Edge,</span>tested globally. Â  </p><p id=p79>Slow CryptoThe <span class='token pointer' id='tok-121-6' style='color:red'>PBKDF2</span><span class='token pointer' id='tok-121-7' style='color:red'>algorithm</span>is designed to be slow to <span class='token pointer' id='tok-121-8' style='color:red'>compute.</span>It's used to hash passwords; its slowness makes it harder for a password cracker to do their job. Its extreme CPU usage also makes it a good benchmark for the CPU performance of a service like <span class='token pointer' id='tok-121-9' style='color:red'>Lambda</span>or Cloudflare Workers. </p><p id=p130>We've written a test based on the <span class='token pointer' id='tok-121-10' style='color:red'>Node Crypto (Lambda)</span>the <span class='token pointer' id='tok-121-11' style='color:red'>APIs.</span><span class='token pointer' id='tok-121-12' style='color:red'>WebCrypto (Workers)</span>Lambda is deployed to with the default <span class='token pointer' id='tok-121-13' style='color:red'>128MB</span>of <span class='token pointer' id='tok-121-14' style='color:red'>memory</span>behind an <span class='token pointer' id='tok-121-15' style='color:red'>API</span>Gateway in us-east-1, our Worker is, as always, deployed around the world. I also have our function running in a Lambda@Edge <span class='token pointer' id='tok-121-16' style='color:red'>deployment</span>to compare that performance as well. Again, we're using <span class='token pointer' id='tok-121-17' style='color:red'>Catchpoint</span>to test from hundreds of locations around the world. </p><p id=p200>The 95th <span class='token pointer' id='tok-121-18' style='color:red'>percentile</span>speed of Workers is <span class='token pointer' id='tok-121-19' style='color:red'>242ms.</span>Lambda@Edge is <span class='token pointer' id='tok-121-20' style='color:red'>842ms</span>(2.4x slower), Lambda <span class='token pointer' id='tok-121-21' style='color:red'>1346ms</span>(4.6x slower). </p><p id=p217>Lambdas are billed based on the amount of memory they allocate and the number of CPU ms they consume, rounded up to the nearest hundred ms. Running a function which consumes <span class='token pointer' id='tok-121-22' style='color:red'>800ms</span>of compute will cost me $1.86 per million requests. Workers is $0.50/million flat. Obviously even beyond the cost, my users would have a pretty terrible experience waiting over a second for responses. </p><p id=p281>As I said, Workers run in almost 160 locations around the world while my Lambda is only running in Northern Virgina. This is something of an unfair comparison, so let's look at just tests in Washington DC: </p><p id=p318>Unsurprisingly the Lambda and Lambda@Edge performance evens out to show us exactly what speed Amazon throttles our CPU to. At the 50th percentile, Lambda processors appear to be 6-7x slower than the mid-range <span class='token pointer' id='tok-121-23' style='color:red'>server cores</span>our <span class='token pointer' id='tok-121-24' style='color:red'>data centers.</span></p><p id=p358>MemoryWhat you might not realize is that the power of the CPU your Lambda gets is a function of how much memory you allocate for it. As you ramp up the memory you get more performance. To test this we allocated a function with <span class='token pointer' id='tok-121-25' style='color:red'>1024MB</span>of memory: </p><p id=p405>Again this are only tests emanating from Washington DC, and I have filtered points over 500ms. </p><p id=p421>The performance of both the Lambda and Lambda@Edge tests with 1024MB of memory is now much closer, but remains roughly half that of Workers. Even worse, running my new Lambda through the somewhat <span class='token pointer' id='tok-121-26' style='color:red'>byzantine pricing formula</span>that 100ms (the minimum) of my 1024MB Lambda will cost me the exact same $1.86 I was paying before. This makes Lambda 3.7x more expensive than Workers on a per-cycle basis. </p><p id=p489>Adding more memory than that only adds cost, not speed. Here is a 3008MB instance (the max I can allocate): </p><p id=p509>This Lambda would be costing me $5.09 per million requests, over 10x more than a Worker, but would still provide less CPU. </p><p id=p531>In general our supposition is a 128MB Lambda is buying you 1/8th of a CPU core. This scales up as you elect to purchase <span class='token pointer' id='tok-121-27' style='color:red'>Lambda's</span>with more memory, until it hits the performance of one of their cores (minus <span class='token pointer' id='tok-121-28' style='color:red'>hypervisor overhead)</span>which point it levels off. </p><p id=p578>Ultimately Amazon is charging based on the duration of execution, but on top of a much slower execution platform than we expected. </p><p id=p600>Native <span class='token pointer' id='tok-121-29' style='color:red'>CodeOne</span>response we saw to our last post was that Lambda supports running native <span class='token pointer' id='tok-121-30' style='color:red'>binaries</span>and that's where its real performance will be exhibited. </p><p id=p625>It is of course true that our <span class='token pointer' id='tok-121-31' style='color:red'>Javascript</span>tests are likely (definitely in the case of Workers) just calling out to an underlying compiled cryptography implementation. But I don't know the details of how Lambda is implemented, perhaps the critics have a point that native binaries could be faster. So I decided to extend my tests. </p><p id=p681>After beseeching a couple of my colleagues who have more recent <span class='token pointer' id='tok-121-32' style='color:red'>C++</span>experience than I do, I ended up with a Lambda which executes the <span class='token pointer' id='tok-121-33' style='color:red'>BoringSSL implementation</span>PBKDF2 in plain <span class='token pointer' id='tok-121-34' style='color:red'>C++14.</span>The results are utterly boring: </p><p id=p718>A Lambda executing native code is just as slow as one executing Javascript. </p><p id=p731><span class='token pointer' id='tok-121-35' style='color:red'>JavaAs</span>I mentioned, irrespective of the language, all of this cryptography is likely calling the same highly optimized C implementations. In that way it might not be an accurate reflection of the performance of our chosen <span class='token pointer' id='tok-121-36' style='color:red'>programming languages</span>is an accurate reflection of CPU performance). Many people still believe that code written in languages like <span class='token pointer' id='tok-121-37' style='color:red'>Java</span>is faster than Javascript in V8. I decided to disprove that as well, and I was willing to sacrifice to the point where I installed the <span class='token pointer' id='tok-121-38' style='color:red'>JDK,</span>and <span class='token pointer' id='tok-121-39' style='color:red'>resigned myself to a lifetime</span>update notifications. </p><p id=p824>To test <span class='token pointer' id='tok-121-40' style='color:red'>application-level</span>performance I dusted off my interview chops and wrote a <span class='token pointer' id='tok-121-41' style='color:red'>prime factorization function</span>both Javascript and Java. I won't weigh in on the wisdom of using new vs <span class='token pointer' id='tok-121-42' style='color:red'>old guard languages,</span>I will say it doesn't buy you much with Lambda: </p><p id=p871>This is charting two Lambdas, one with 128MB of memory, the other 1024MB.  The tests are all from Washington, DC (near both of our Lambdas) <span class='token pointer' id='tok-121-43' style='color:red'>to eliminate the advantage of</span>Worker's global presence. The 128MB instance has <span class='token pointer' id='tok-121-44' style='color:red'>1745ms of 95% percentile</span>time. </p><p id=p916>When we look at tests which originate all around the globe, where billions of <span class='token pointer' id='tok-121-45' style='color:red'>Internet</span>users browse every day, we get the best example of the power of a Worker's deployment yet: </p><p id=p948>When we exclude the competitors from our analysis we are able to use the same techniques to examine the performance of the Workers platform itself and uncover more than a few opportunities for improvement and <span class='token pointer' id='tok-121-46' style='color:red'>optimization.</span>That's what we're focused on: making Workers even faster, more powerful, and easier to use.
 </p>