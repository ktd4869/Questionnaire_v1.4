<p id=p0>In a previous post we discussed the performance limitations of the <span class='token pointer' id='tok-118-0' style='color:red'>kernel</span><span class='token pointer' id='tok-118-1' style='color:red'>Linux</span>stack. We detailed the available <span class='token pointer' id='tok-118-2' style='color:red'>kernel bypass techniques</span>user space programs to receive <span class='token pointer' id='tok-118-3' style='color:red'>packets</span>with high throughput. Unfortunately, none of the discussed <span class='token pointer' id='tok-118-4' style='color:red'>supported</span><span class='token pointer' id='tok-118-5' style='color:red'>open source solutions</span>needs. To improve the situation we decided to contribute to the <span class='token pointer' id='tok-118-6' style='color:red'>Netmap</span>project. In this blog post we'll describe our proposed changes. </p><p id=p64>CC BY-SA 2.0 image by </p><p id=p71>Our needsAt <span class='token pointer' id='tok-118-7' style='color:red'>CloudFlare</span>we are constantly dealing with <span class='token pointer' id='tok-118-8' style='color:red'>packet</span><span class='token pointer' id='tok-118-9' style='color:red'>large</span>Our <span class='token pointer' id='tok-118-10' style='color:red'>network</span>constantly receives a large volume of packets, often coming from many, simultaneous attacks. In fact, it is entirely possible that the server which just served you this blog post is dealing with a many-million packets per second flood right now. </p><p id=p124>Since the <span class='token pointer' id='tok-118-11' style='color:red'>Linux Kernel</span>really handle a large volume of packets, we need to work around it. During packet floods we <span class='token pointer' id='tok-118-12' style='color:red'>offload</span>selected network flows (belonging to a flood) to a user space application. This application filters the packets at very high speed. Most of the packets are dropped, as they  belong to a flood. The small number of "valid" packets are injected <span class='token pointer' id='tok-118-13' style='color:red'>back</span>to the kernel and handled in the same way as usual traffic. </p><p id=p202>It's important to emphasize that the <span class='token pointer' id='tok-118-14' style='color:red'>kernel bypass</span>enabled only for selected flows, which means that all other packets go to the kernel as usual. </p><p id=p228>This setup works perfectly on our servers with <span class='token pointer' id='tok-118-15' style='color:red'>Solarflare</span>network cards - we can use the ef_vi <span class='token pointer' id='tok-118-16' style='color:red'>API</span>to achieve the kernel bypass. Unfortunately, we don't have this functionality on our servers with Intel IXGBE NIC's. </p><p id=p264>This is when Netmap comes in. </p><p id=p270>NetmapOver the last few months we've been thinking hard about how to achieve bypass for selected flows (aka: <span class='token pointer' id='tok-118-17' style='color:red'>bifurcated driver)</span><span class='token pointer' id='tok-118-18' style='color:red'>non-Solarflare</span>network cards. </p><p id=p294>We've considered <span class='token pointer' id='tok-118-19' style='color:red'>PF_RING,</span><span class='token pointer' id='tok-118-20' style='color:red'>DPDK</span>and other custom solutions, but sadly all of them take over the whole network card. Eventually we decided that the best way would be to patch Netmap with the functionality we need. </p><p id=p330>We chose Netmap because: </p><p id=p334>It's fully <span class='token pointer' id='tok-118-21' style='color:red'>open source</span>released under a <span class='token pointer' id='tok-118-22' style='color:red'>BSD</span>license.It has a great <span class='token pointer' id='tok-118-23' style='color:red'>NIC-agnostic</span><span class='token pointer' id='tok-118-24' style='color:red'>API.It's</span>very fast: can reach line rate easily.The project is well maintained and reasonably mature.The code is very high quality.The driver-specific modifications are trivial: most of the magic happens in the shared Netmap module. It's easy to add support for new hardware.Introducing the single <span class='token pointer' id='tok-118-25' style='color:red'>RX queue</span>when a network card goes into the Netmap mode, all the <span class='token pointer' id='tok-118-26' style='color:red'>queues</span><span class='token pointer' id='tok-118-27' style='color:red'>RX</span>disconnected from the kernel and are available to the Netmap applications. </p><p id=p420>We don't want that. We want to keep most of the RX queues back in the kernel mode, and enable Netmap mode only on selected RX queues. We call this functionality: "single RX queue mode". </p><p id=p455>The intention was to expose a minimal API which could: </p><p id=p465>Open a network interface in "a single RX queue mode".This would allow netmap applications to receive packets from that specific <span class='token pointer' id='tok-118-28' style='color:red'>RX queue.While</span>all the other queues attached to the host network stack.On demand add or remove RX queues from the "single RX queue mode".Eventually remove the interface from the Netmap mode and <span class='token pointer' id='tok-118-29' style='color:red'>reattach</span>the RX queues to the host stack. </p><p id=p526>Indeed the only difference is the nm_open() call, which uses the new syntax netmap:ifname~queue_number. </p><p id=p540>Once again, when running this code only packets arriving on the RX queue #4 will go to the netmap program. All other RX and TX queues will be handled by the Linux kernel network stack. </p><p id=p575>Isolating a queueIn multiqueue network cards, any packet can end up in almost any RX queue due to <span class='token pointer' id='tok-118-30' style='color:red'>RSS.</span>This is why before enabling the single RX mode it is necessary to make sure only the selected flow goes to the Netmap queue. </p><p id=p618>To do so it is necessary to: </p><p id=p625>Modify the indirection table to ensure no new <span class='token pointer' id='tok-118-31' style='color:red'>RSS-hashed</span>packets will go there.Use <span class='token pointer' id='tok-118-32' style='color:red'>flow steering</span>specifically direct some flows to the isolated queue.Work around RFS - make sure no other application is running on the CPU Netmap will run on.For example: </p><p id=p667>$ <span class='token pointer' id='tok-118-33' style='color:red'>ethtool</span>-X eth3 weight 1 1 1 1 0 1 1 1 1 1$ ethtool -K eth3 ntuple on$ ethtool -N eth3 flow-type udp4 <span class='token pointer' id='tok-118-34' style='color:red'>dst-port</span>53 action 4Here we are setting the indirection table to prevent traffic from going to RX queue #4. Then we are enabling flow steering to enqueue all <span class='token pointer' id='tok-118-35' style='color:red'>UDP traffic</span>destination <span class='token pointer' id='tok-118-36' style='color:red'>port</span>53 into queue #4. </p><p id=p729>Trying it outHere's how to run it with the IXGBE NIC. First grab the sources: </p><p id=p744>$ git clone https://github.com/jibi/netmap.git$ cd netmap$ git checkout -B single-rx-queue-mode$ ./configure --drivers=ixgbe <span class='token pointer' id='tok-118-37' style='color:red'>--kernel-sources=/path/to/kernelLoad</span>the netmap-patched modules and setup the interface: </p><p id=p764>$ insmod ./LINUX/netmap.ko$ insmod ./LINUX/ixgbe/ixgbe.ko$ # Distribute the interrupts:$ (let CPU=0; cd /sys/class/net/eth3/device/msi_irqs/; for IRQ in *; do \  echo $CPU > /proc/irq/$IRQ/smp_affinity_list; let CPU+=1         done)$ # Enable RSS:$ ethtool -K eth3 ntuple onAt this point we started flooding the interface with 6M short UDP packets. htop shows the server being <span class='token pointer' id='tok-118-38' style='color:red'>totally</span>busy with handling the flood: </p><p id=p830>To counter the flood we started Netmap. First, we needed to edit the indirection table, to isolate the RX queue #4: </p><p id=p851>$ ethtool -X eth3 weight 1 1 1 1 0 1 1 1 1 1$ ethtool -N eth3 flow-type udp4 dst-port 53 action 4This caused all the flood packets to go to RX queue #4. </p><p id=p886>Before putting an interface in Netmap mode it is necessary to turn off hardware offload features: </p><p id=p902>$ ethtool -K eth3 lro off gro offFinally we launched the netmap offload: </p><p id=p915>$ sudo taskset -c 15 ./nm_offload eth3 4[+] starting test02 on interface eth3 ring 4[+] UDP pps: 5844714[+] UDP pps: 5996166[+] UDP pps: 5863214[+] UDP pps: 5986365[+] UDP pps: 5867302[+] UDP pps: 5964911[+] UDP pps: 5909715[+] UDP pps: 5865769[+] UDP pps: 5906668[+] UDP pps: 5875486As you see the netmap program on a single RX queue was able to receive about 5.8M packets. </p><p id=p977>For completeness, here's an htop showing only a single core being busy with Netmap: </p><p id=p991>ThanksWe would like to thank Pavel Odintsov who suggested the possibility of using Netmap this way. He even prepared the initial hack we based our work on. </p><p id=p1018>We would also like to thank Luigi Rizzo, for his Netmap work and great feedback on our <span class='token pointer' id='tok-118-39' style='color:red'>patches.</span></p><p id=p1036>Final wordsAt CloudFlare our application stack is based on open source software. We're grateful to so many open source programmers for their awesome work. Whenever we can we try to contribute back to the community - we hope "the single RX Netmap mode" will be useful to others. </p><p id=p1084>You can find more CloudFlare open source here.
 </p>